{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ładowanie modeli\n",
    "Ładowanie dwóch modeli, które nauczyliśmy wraz z ich najlepszymi wagami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
      "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
      "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
      "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
      "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
      "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
      "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
      "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from custom_layers import BatchNormalization, Softmax4D\n",
    "\n",
    "segnet_path = \"modeleAF/model-e6dc8464-7322-4237-b669-95e4ad5819cb/\"\n",
    "#https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\n",
    "unet_path = \"model-a679583b-da1c-437e-aaa4-58ef1f9e4f58/\"\n",
    "\n",
    "#load models\n",
    "with open(segnet_path + 'architecture.json', 'r') as json:\n",
    "    segnet_load = json.read()\n",
    "segnet = model_from_json(segnet_load, {\"Softmax4D\": Softmax4D, \n",
    "                                       \"BatchNormalization\": BatchNormalization})\n",
    "\n",
    "with open(unet_path + 'architecture.json', 'r') as json:\n",
    "    unet_load = json.read()\n",
    "    \n",
    "unet = model_from_json(unet_load, {\"Softmax4D\": Softmax4D})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights\n",
    "segnet.load_weights(segnet_path + \"weights.87-0.98.hdf5\")\n",
    "unet.load_weights(unet_path + \"weights.15-0.78.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, None, 64)    256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, None, 128)   512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, None, 256)   1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, None, None, 256)   1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, None, None, 256)   1024      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, None, None, 128)   295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, None, None, 128)   512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, None, None, 64)    73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, None, None, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, None, None, 2)     130       \n",
      "_________________________________________________________________\n",
      "softmax4d (Softmax4D)        (None, None, None, 2)     0         \n",
      "=================================================================\n",
      "Total params: 5,467,330\n",
      "Trainable params: 5,463,490\n",
      "Non-trainable params: 3,840\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "segnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 512, 512, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 512, 512, 64) 36928       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 256, 256, 64) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 256, 256, 128 73856       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 256, 256, 128 147584      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 128 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 256 295168      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 256 590080      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 256)  0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 512)  1180160     max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 512)  2359808     conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 128, 128, 512 0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 128, 128, 256 524544      up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128, 128, 512 0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 128, 128, 256 1179904     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 128, 128, 256 590080      conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 256, 256, 256 0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 256, 256, 128 131200      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256, 256, 256 0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 256, 256, 128 295040      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 256, 256, 128 147584      conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 512, 512, 128 0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 512, 512, 64) 32832       up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512, 512, 128 0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 512, 512, 64) 73792       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 512, 512, 2)  1154        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 512, 512, 1)  3           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax4d (Softmax4D)           (None, 512, 512, 1)  0           conv2d_18[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,698,437\n",
      "Trainable params: 7,698,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "unet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Segnet* osiągnał powyżej 0.97 ważonej celności, a *U-Net* 0.78."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segnet.trainable = False\n",
    "len(segnet.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.trainable = False\n",
    "len(unet.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_layers_names(model, name):\n",
    "    for layer in model.layers:\n",
    "        layer._name = name + \"_\" + layer.name\n",
    "        \n",
    "change_layers_names(segnet, \"segnet\")\n",
    "change_layers_names(unet, \"unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate, Conv2D\n",
    "\n",
    "ensemble_input = [segnet.input, unet.input]\n",
    "ensemble_output = [segnet.input, unet.input]\n",
    "\n",
    "merge = Concatenate()(ensemble_output)\n",
    "conv = Conv2D(2, (1, 1))(merge)\n",
    "out = Softmax4D()(conv)\n",
    "\n",
    "ensemble = Model(inputs=ensemble_input, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_metrics as M\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class_weights = [1516.576, 0.1]\n",
    "unsuper_weight = 0.1\n",
    "\n",
    "loss = M.loss(weights=class_weights, unsuper_weight=unsuper_weight, unsuper_channel=-1)\n",
    "wcce = M.wcceOA(weights=class_weights)\n",
    "uentr = M.entrONA(unsuper_channel=-1)\n",
    "wacc = M.waccOA(weights=class_weights)\n",
    "\n",
    "ensemble.compile(optimizer=Adam(1e-4), loss=loss, metrics=[wcce, uentr, wacc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-27b8ee78-d5aa-4dbd-a4bf-62726e011c16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-Komputer\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "400/400 [==============================] - 55s 101ms/step - loss: 3.1213 - wcceOA: 3.4543 - entrONA: 0.1235 - waccOA: 0.6367 - val_loss: 0.7441 - val_wcceOA: 0.7630 - val_entrONA: 0.5747 - val_waccOA: 0.3881\n",
      "\n",
      "Epoch 00001: val_waccOA improved from -inf to 0.38809, saving model to model-27b8ee78-d5aa-4dbd-a4bf-62726e011c16\\weights.01-0.39.hdf5\n",
      "Epoch 2/1000\n",
      "400/400 [==============================] - 13s 32ms/step - loss: 0.7092 - wcceOA: 0.7194 - entrONA: 0.6179 - waccOA: 0.5948 - val_loss: 0.6978 - val_wcceOA: 0.7058 - val_entrONA: 0.6254 - val_waccOA: 0.4146\n",
      "\n",
      "Epoch 00002: val_waccOA improved from 0.38809 to 0.41457, saving model to model-27b8ee78-d5aa-4dbd-a4bf-62726e011c16\\weights.02-0.41.hdf5\n",
      "Epoch 3/1000\n",
      "400/400 [==============================] - 14s 34ms/step - loss: 0.7100 - wcceOA: 0.7204 - entrONA: 0.6163 - waccOA: 0.6154 - val_loss: 0.6594 - val_wcceOA: 0.6573 - val_entrONA: 0.6786 - val_waccOA: 0.6414\n",
      "\n",
      "Epoch 00003: val_waccOA improved from 0.41457 to 0.64145, saving model to model-27b8ee78-d5aa-4dbd-a4bf-62726e011c16\\weights.03-0.64.hdf5\n",
      "Epoch 4/1000\n",
      "400/400 [==============================] - 11s 28ms/step - loss: 0.6823 - wcceOA: 0.6889 - entrONA: 0.6238 - waccOA: 0.6485 - val_loss: 0.8135 - val_wcceOA: 0.8549 - val_entrONA: 0.4405 - val_waccOA: 0.6980\n",
      "\n",
      "Epoch 00004: val_waccOA improved from 0.64145 to 0.69796, saving model to model-27b8ee78-d5aa-4dbd-a4bf-62726e011c16\\weights.04-0.70.hdf5\n",
      "Epoch 5/1000\n",
      "400/400 [==============================] - 11s 27ms/step - loss: 0.6679 - wcceOA: 0.6745 - entrONA: 0.6085 - waccOA: 0.6640 - val_loss: 0.6821 - val_wcceOA: 0.6915 - val_entrONA: 0.5972 - val_waccOA: 0.4695\n",
      "\n",
      "Epoch 00005: val_waccOA did not improve from 0.69796\n",
      "Epoch 6/1000\n",
      "400/400 [==============================] - 11s 27ms/step - loss: 0.6548 - wcceOA: 0.6587 - entrONA: 0.6193 - waccOA: 0.6872 - val_loss: 0.6576 - val_wcceOA: 0.6618 - val_entrONA: 0.6199 - val_waccOA: 0.5290\n",
      "\n",
      "Epoch 00006: val_waccOA did not improve from 0.69796\n",
      "Epoch 7/1000\n",
      "400/400 [==============================] - 11s 28ms/step - loss: 0.6573 - wcceOA: 0.6620 - entrONA: 0.6155 - waccOA: 0.6895 - val_loss: 0.6437 - val_wcceOA: 0.6490 - val_entrONA: 0.5965 - val_waccOA: 0.5640\n",
      "\n",
      "Epoch 00007: val_waccOA did not improve from 0.69796\n",
      "Epoch 8/1000\n",
      "400/400 [==============================] - 10s 25ms/step - loss: 0.6497 - wcceOA: 0.6569 - entrONA: 0.5856 - waccOA: 0.7140 - val_loss: 0.6207 - val_wcceOA: 0.6184 - val_entrONA: 0.6415 - val_waccOA: 0.6814\n",
      "\n",
      "Epoch 00008: val_waccOA did not improve from 0.69796\n",
      "Epoch 9/1000\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.6383 - wcceOA: 0.6418 - entrONA: 0.6071 - waccOA: 0.7150 - val_loss: 0.6068 - val_wcceOA: 0.5995 - val_entrONA: 0.6731 - val_waccOA: 0.7436\n",
      "\n",
      "Epoch 00009: val_waccOA improved from 0.69796 to 0.74359, saving model to model-27b8ee78-d5aa-4dbd-a4bf-62726e011c16\\weights.09-0.74.hdf5\n",
      "Epoch 10/1000\n",
      "400/400 [==============================] - 9s 24ms/step - loss: 0.6307 - wcceOA: 0.6321 - entrONA: 0.6189 - waccOA: 0.7340 - val_loss: 0.6871 - val_wcceOA: 0.7075 - val_entrONA: 0.5033 - val_waccOA: 0.5026\n",
      "\n",
      "Epoch 00010: val_waccOA did not improve from 0.74359\n",
      "Epoch 11/1000\n",
      "400/400 [==============================] - 9s 23ms/step - loss: 0.6224 - wcceOA: 0.6255 - entrONA: 0.5946 - waccOA: 0.7319 - val_loss: 0.6397 - val_wcceOA: 0.6483 - val_entrONA: 0.5624 - val_waccOA: 0.5630\n",
      "\n",
      "Epoch 00011: val_waccOA did not improve from 0.74359\n",
      "Epoch 12/1000\n",
      "400/400 [==============================] - 9s 23ms/step - loss: 0.6188 - wcceOA: 0.6218 - entrONA: 0.5917 - waccOA: 0.7161 - val_loss: 0.5905 - val_wcceOA: 0.5820 - val_entrONA: 0.6665 - val_waccOA: 0.7734\n",
      "\n",
      "Epoch 00012: val_waccOA improved from 0.74359 to 0.77335, saving model to model-27b8ee78-d5aa-4dbd-a4bf-62726e011c16\\weights.12-0.77.hdf5\n",
      "Epoch 13/1000\n",
      "400/400 [==============================] - 9s 22ms/step - loss: 0.6065 - wcceOA: 0.6094 - entrONA: 0.5798 - waccOA: 0.7325 - val_loss: 0.6140 - val_wcceOA: 0.6192 - val_entrONA: 0.5670 - val_waccOA: 0.6196\n",
      "\n",
      "Epoch 00013: val_waccOA did not improve from 0.77335\n",
      "Epoch 14/1000\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.6053 - wcceOA: 0.6066 - entrONA: 0.5935 - waccOA: 0.7496 - val_loss: 0.6065 - val_wcceOA: 0.6126 - val_entrONA: 0.5511 - val_waccOA: 0.6240\n",
      "\n",
      "Epoch 00014: val_waccOA did not improve from 0.77335\n",
      "Epoch 15/1000\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.6096 - wcceOA: 0.6148 - entrONA: 0.5624 - waccOA: 0.7070 - val_loss: 0.6137 - val_wcceOA: 0.6209 - val_entrONA: 0.5486 - val_waccOA: 0.7450 0.6147 - \n",
      "\n",
      "Epoch 00015: val_waccOA did not improve from 0.77335\n",
      "Epoch 16/1000\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 0.5881 - wcceOA: 0.5875 - entrONA: 0.5933 - waccOA: 0.7534 - val_loss: 0.6228 - val_wcceOA: 0.6337 - val_entrONA: 0.5245 - val_waccOA: 0.5903\n",
      "\n",
      "Epoch 00016: val_waccOA did not improve from 0.77335\n",
      "Epoch 17/1000\n",
      "400/400 [==============================] - 11s 28ms/step - loss: 0.5961 - wcceOA: 0.5982 - entrONA: 0.5773 - waccOA: 0.7400 - val_loss: 0.5997 - val_wcceOA: 0.5990 - val_entrONA: 0.6061 - val_waccOA: 0.7187\n",
      "\n",
      "Epoch 00017: val_waccOA did not improve from 0.77335\n",
      "Epoch 18/1000\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 0.5793 - wcceOA: 0.5790 - entrONA: 0.5818 - waccOA: 0.7578 - val_loss: 0.5586 - val_wcceOA: 0.5494 - val_entrONA: 0.6417 - val_waccOA: 0.7694\n",
      "\n",
      "Epoch 00018: val_waccOA did not improve from 0.77335\n",
      "Epoch 19/1000\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 0.5717 - wcceOA: 0.5711 - entrONA: 0.5771 - waccOA: 0.7535 - val_loss: 0.5654 - val_wcceOA: 0.5637 - val_entrONA: 0.5811 - val_waccOA: 0.7903\n",
      "\n",
      "Epoch 00019: val_waccOA improved from 0.77335 to 0.79034, saving model to model-27b8ee78-d5aa-4dbd-a4bf-62726e011c16\\weights.19-0.79.hdf5\n",
      "Epoch 20/1000\n",
      "400/400 [==============================] - 17s 44ms/step - loss: 0.5866 - wcceOA: 0.5879 - entrONA: 0.5752 - waccOA: 0.7361 - val_loss: 0.5196 - val_wcceOA: 0.5068 - val_entrONA: 0.6351 - val_waccOA: 0.8195\n",
      "\n",
      "Epoch 00020: val_waccOA improved from 0.79034 to 0.81952, saving model to model-27b8ee78-d5aa-4dbd-a4bf-62726e011c16\\weights.20-0.82.hdf5\n",
      "Epoch 21/1000\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.5800 - wcceOA: 0.5805 - entrONA: 0.5749 - waccOA: 0.7416 - val_loss: 0.5437 - val_wcceOA: 0.5398 - val_entrONA: 0.5785 - val_waccOA: 0.8278\n",
      "\n",
      "Epoch 00021: val_waccOA improved from 0.81952 to 0.82781, saving model to model-27b8ee78-d5aa-4dbd-a4bf-62726e011c16\\weights.21-0.83.hdf5\n",
      "Epoch 22/1000\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.5771 - wcceOA: 0.5757 - entrONA: 0.5892 - waccOA: 0.7460 - val_loss: 0.5355 - val_wcceOA: 0.5257 - val_entrONA: 0.6237 - val_waccOA: 0.7831.5783 - wcceOA: 0.5771 - entrONA: 0.5892 \n",
      "\n",
      "Epoch 00022: val_waccOA did not improve from 0.82781\n",
      "Epoch 23/1000\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.5643 - wcceOA: 0.5627 - entrONA: 0.5793 - waccOA: 0.7636 - val_loss: 0.5422 - val_wcceOA: 0.5366 - val_entrONA: 0.5923 - val_waccOA: 0.8605\n",
      "\n",
      "Epoch 00023: val_waccOA improved from 0.82781 to 0.86050, saving model to model-27b8ee78-d5aa-4dbd-a4bf-62726e011c16\\weights.23-0.86.hdf5\n",
      "Epoch 24/1000\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.5568 - wcceOA: 0.5548 - entrONA: 0.5748 - waccOA: 0.7701 - val_loss: 0.5585 - val_wcceOA: 0.5611 - val_entrONA: 0.5351 - val_waccOA: 0.7301\n",
      "\n",
      "Epoch 00024: val_waccOA did not improve from 0.86050\n",
      "Epoch 25/1000\n",
      "400/400 [==============================] - 25s 63ms/step - loss: 0.5378 - wcceOA: 0.5335 - entrONA: 0.5766 - waccOA: 0.7813 - val_loss: 0.5941 - val_wcceOA: 0.6032 - val_entrONA: 0.5122 - val_waccOA: 0.6471\n",
      "\n",
      "Epoch 00025: val_waccOA did not improve from 0.86050\n",
      "Epoch 26/1000\n",
      "400/400 [==============================] - 17s 43ms/step - loss: 0.5580 - wcceOA: 0.5598 - entrONA: 0.5416 - waccOA: 0.7673 - val_loss: 0.5978 - val_wcceOA: 0.6103 - val_entrONA: 0.4850 - val_waccOA: 0.6364\n",
      "\n",
      "Epoch 00026: val_waccOA did not improve from 0.86050\n",
      "Epoch 27/1000\n",
      "400/400 [==============================] - 22s 54ms/step - loss: 0.5542 - wcceOA: 0.5556 - entrONA: 0.5420 - waccOA: 0.7749 - val_loss: 0.5156 - val_wcceOA: 0.5059 - val_entrONA: 0.6033 - val_waccOA: 0.7836\n",
      "\n",
      "Epoch 00027: val_waccOA did not improve from 0.86050\n",
      "Epoch 28/1000\n",
      "400/400 [==============================] - 24s 59ms/step - loss: 0.5479 - wcceOA: 0.5466 - entrONA: 0.5600 - waccOA: 0.7763 - val_loss: 0.5533 - val_wcceOA: 0.5574 - val_entrONA: 0.5165 - val_waccOA: 0.7304\n",
      "\n",
      "Epoch 00028: val_waccOA did not improve from 0.86050\n",
      "Epoch 29/1000\n",
      "400/400 [==============================] - 19s 49ms/step - loss: 0.5470 - wcceOA: 0.5457 - entrONA: 0.5589 - waccOA: 0.7725 - val_loss: 0.5224 - val_wcceOA: 0.5145 - val_entrONA: 0.5934 - val_waccOA: 0.7881\n",
      "\n",
      "Epoch 00029: val_waccOA did not improve from 0.86050\n",
      "Epoch 30/1000\n",
      "400/400 [==============================] - 13s 32ms/step - loss: 0.5377 - wcceOA: 0.5358 - entrONA: 0.5545 - waccOA: 0.7850 - val_loss: 0.5440 - val_wcceOA: 0.5459 - val_entrONA: 0.5266 - val_waccOA: 0.7632\n",
      "\n",
      "Epoch 00030: val_waccOA did not improve from 0.86050\n",
      "Epoch 31/1000\n",
      "400/400 [==============================] - 14s 36ms/step - loss: 0.5476 - wcceOA: 0.5490 - entrONA: 0.5354 - waccOA: 0.7746 - val_loss: 0.5394 - val_wcceOA: 0.5356 - val_entrONA: 0.5737 - val_waccOA: 0.7420OA: 0.5491 - entrONA: 0.5354 - waccOA: 0\n",
      "\n",
      "Epoch 00031: val_waccOA did not improve from 0.86050\n",
      "Epoch 32/1000\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.5645 - wcceOA: 0.5677 - entrONA: 0.5360 - waccOA: 0.7530 - val_loss: 0.5177 - val_wcceOA: 0.5126 - val_entrONA: 0.5638 - val_waccOA: 0.8155\n",
      "\n",
      "Epoch 00032: val_waccOA did not improve from 0.86050\n",
      "Epoch 33/1000\n",
      "400/400 [==============================] - 10s 26ms/step - loss: 0.5475 - wcceOA: 0.5482 - entrONA: 0.5420 - waccOA: 0.7681 - val_loss: 0.5085 - val_wcceOA: 0.5017 - val_entrONA: 0.5696 - val_waccOA: 0.7893\n",
      "\n",
      "Epoch 00033: val_waccOA did not improve from 0.86050\n",
      "Epoch 34/1000\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 0.5121 - wcceOA: 0.5080 - entrONA: 0.5494 - waccOA: 0.8063 - val_loss: 0.5608 - val_wcceOA: 0.5687 - val_entrONA: 0.4890 - val_waccOA: 0.7126\n",
      "\n",
      "Epoch 00034: val_waccOA did not improve from 0.86050\n",
      "Epoch 35/1000\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 0.5576 - wcceOA: 0.5627 - entrONA: 0.5118 - waccOA: 0.7520 - val_loss: 0.5188 - val_wcceOA: 0.5146 - val_entrONA: 0.5561 - val_waccOA: 0.7788\n",
      "\n",
      "Epoch 00035: val_waccOA did not improve from 0.86050\n",
      "Epoch 36/1000\n",
      "400/400 [==============================] - 11s 27ms/step - loss: 0.5014 - wcceOA: 0.4970 - entrONA: 0.5404 - waccOA: 0.8095 - val_loss: 0.5569 - val_wcceOA: 0.5625 - val_entrONA: 0.5063 - val_waccOA: 0.7482\n",
      "\n",
      "Epoch 00036: val_waccOA did not improve from 0.86050\n",
      "Epoch 37/1000\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 0.5275 - wcceOA: 0.5266 - entrONA: 0.5348 - waccOA: 0.7800 - val_loss: 0.7080 - val_wcceOA: 0.7507 - val_entrONA: 0.3236 - val_waccOA: 0.7455\n",
      "\n",
      "Epoch 00037: val_waccOA did not improve from 0.86050\n",
      "Epoch 38/1000\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 0.5634 - wcceOA: 0.5704 - entrONA: 0.4998 - waccOA: 0.7408 - val_loss: 0.5139 - val_wcceOA: 0.5108 - val_entrONA: 0.5422 - val_waccOA: 0.8454\n",
      "\n",
      "Epoch 00038: val_waccOA did not improve from 0.86050\n",
      "Epoch 39/1000\n",
      "400/400 [==============================] - 14s 36ms/step - loss: 0.5535 - wcceOA: 0.5574 - entrONA: 0.5182 - waccOA: 0.7557 - val_loss: 0.5074 - val_wcceOA: 0.5011 - val_entrONA: 0.5646 - val_waccOA: 0.7854\n",
      "\n",
      "Epoch 00039: val_waccOA did not improve from 0.86050\n",
      "Epoch 40/1000\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.5042 - wcceOA: 0.5013 - entrONA: 0.5303 - waccOA: 0.8016 - val_loss: 0.5203 - val_wcceOA: 0.5196 - val_entrONA: 0.5273 - val_waccOA: 0.8641\n",
      "\n",
      "Epoch 00040: val_waccOA improved from 0.86050 to 0.86412, saving model to model-27b8ee78-d5aa-4dbd-a4bf-62726e011c16\\weights.40-0.86.hdf5\n",
      "Epoch 41/1000\n",
      "400/400 [==============================] - 20s 50ms/step - loss: 0.5473 - wcceOA: 0.5491 - entrONA: 0.5304 - waccOA: 0.7778 - val_loss: 0.4905 - val_wcceOA: 0.4853 - val_entrONA: 0.5366 - val_waccOA: 0.8559\n",
      "\n",
      "Epoch 00041: val_waccOA did not improve from 0.86412\n",
      "Epoch 42/1000\n",
      "400/400 [==============================] - 17s 44ms/step - loss: 0.5330 - wcceOA: 0.5350 - entrONA: 0.5151 - waccOA: 0.7760 - val_loss: 0.5285 - val_wcceOA: 0.5308 - val_entrONA: 0.5079 - val_waccOA: 0.8288\n",
      "\n",
      "Epoch 00042: val_waccOA did not improve from 0.86412\n",
      "Epoch 43/1000\n",
      "400/400 [==============================] - 14s 36ms/step - loss: 0.5058 - wcceOA: 0.5053 - entrONA: 0.5112 - waccOA: 0.8008 - val_loss: 0.5031 - val_wcceOA: 0.4994 - val_entrONA: 0.5359 - val_waccOA: 0.7939\n",
      "\n",
      "Epoch 00043: val_waccOA did not improve from 0.86412\n",
      "Epoch 44/1000\n",
      "400/400 [==============================] - 20s 51ms/step - loss: 0.5005 - wcceOA: 0.4989 - entrONA: 0.5147 - waccOA: 0.8070 - val_loss: 0.5052 - val_wcceOA: 0.5015 - val_entrONA: 0.5393 - val_waccOA: 0.7842\n",
      "\n",
      "Epoch 00044: val_waccOA did not improve from 0.86412\n",
      "Epoch 45/1000\n",
      "400/400 [==============================] - 13s 32ms/step - loss: 0.5311 - wcceOA: 0.5328 - entrONA: 0.5161 - waccOA: 0.7842 - val_loss: 0.6347 - val_wcceOA: 0.6670 - val_entrONA: 0.3435 - val_waccOA: 0.7590\n",
      "\n",
      "Epoch 00045: val_waccOA did not improve from 0.86412\n",
      "Epoch 46/1000\n",
      "400/400 [==============================] - 14s 36ms/step - loss: 0.5508 - wcceOA: 0.5575 - entrONA: 0.4909 - waccOA: 0.7595 - val_loss: 0.4991 - val_wcceOA: 0.4949 - val_entrONA: 0.5374 - val_waccOA: 0.7833\n",
      "\n",
      "Epoch 00046: val_waccOA did not improve from 0.86412\n",
      "Epoch 47/1000\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.5084 - wcceOA: 0.5065 - entrONA: 0.5261 - waccOA: 0.7960 - val_loss: 0.4980 - val_wcceOA: 0.4944 - val_entrONA: 0.5303 - val_waccOA: 0.8000\n",
      "\n",
      "Epoch 00047: val_waccOA did not improve from 0.86412\n",
      "Epoch 48/1000\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.5102 - wcceOA: 0.5097 - entrONA: 0.5139 - waccOA: 0.8035 - val_loss: 0.4957 - val_wcceOA: 0.4909 - val_entrONA: 0.5387 - val_waccOA: 0.7864\n",
      "\n",
      "Epoch 00048: val_waccOA did not improve from 0.86412\n",
      "Epoch 49/1000\n",
      "400/400 [==============================] - 13s 32ms/step - loss: 0.5222 - wcceOA: 0.5246 - entrONA: 0.5005 - waccOA: 0.7917 - val_loss: 0.5328 - val_wcceOA: 0.5401 - val_entrONA: 0.4675 - val_waccOA: 0.7595\n",
      "\n",
      "Epoch 00049: val_waccOA did not improve from 0.86412\n",
      "Epoch 50/1000\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.5420 - wcceOA: 0.5484 - entrONA: 0.4838 - waccOA: 0.7836 - val_loss: 0.4807 - val_wcceOA: 0.4762 - val_entrONA: 0.5212 - val_waccOA: 0.8373\n",
      "\n",
      "Epoch 00050: val_waccOA did not improve from 0.86412\n",
      "Epoch 51/1000\n",
      "400/400 [==============================] - 15s 36ms/step - loss: 0.5162 - wcceOA: 0.5189 - entrONA: 0.4912 - waccOA: 0.7918 - val_loss: 0.5264 - val_wcceOA: 0.5264 - val_entrONA: 0.5262 - val_waccOA: 0.7673\n",
      "\n",
      "Epoch 00051: val_waccOA did not improve from 0.86412\n",
      "Epoch 52/1000\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.5670 - wcceOA: 0.5780 - entrONA: 0.4683 - waccOA: 0.7537 - val_loss: 0.5093 - val_wcceOA: 0.5071 - val_entrONA: 0.5290 - val_waccOA: 0.7857\n",
      "\n",
      "Epoch 00052: val_waccOA did not improve from 0.86412\n",
      "Epoch 53/1000\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.4908 - wcceOA: 0.4889 - entrONA: 0.5079 - waccOA: 0.8149 - val_loss: 0.5838 - val_wcceOA: 0.5988 - val_entrONA: 0.4492 - val_waccOA: 0.7346\n",
      "\n",
      "Epoch 00053: val_waccOA did not improve from 0.86412\n",
      "Epoch 54/1000\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.4968 - wcceOA: 0.4962 - entrONA: 0.5018 - waccOA: 0.8129 - val_loss: 0.5229 - val_wcceOA: 0.5292 - val_entrONA: 0.4665 - val_waccOA: 0.7897\n",
      "\n",
      "Epoch 00054: val_waccOA did not improve from 0.86412\n",
      "Epoch 55/1000\n",
      "400/400 [==============================] - 23s 57ms/step - loss: 0.5094 - wcceOA: 0.5118 - entrONA: 0.4873 - waccOA: 0.8038 - val_loss: 0.6561 - val_wcceOA: 0.6928 - val_entrONA: 0.3253 - val_waccOA: 0.7487\n",
      "\n",
      "Epoch 00055: val_waccOA did not improve from 0.86412\n",
      "Epoch 56/1000\n",
      "400/400 [==============================] - 24s 60ms/step - loss: 0.5427 - wcceOA: 0.5503 - entrONA: 0.4741 - waccOA: 0.7635 - val_loss: 0.4919 - val_wcceOA: 0.4903 - val_entrONA: 0.5065 - val_waccOA: 0.8417\n",
      "\n",
      "Epoch 00056: val_waccOA did not improve from 0.86412\n",
      "Epoch 57/1000\n",
      "400/400 [==============================] - 31s 79ms/step - loss: 0.5119 - wcceOA: 0.5149 - entrONA: 0.4843 - waccOA: 0.8025 - val_loss: 0.5176 - val_wcceOA: 0.5216 - val_entrONA: 0.4812 - val_waccOA: 0.8351\n",
      "\n",
      "Epoch 00057: val_waccOA did not improve from 0.86412\n",
      "Epoch 58/1000\n",
      "400/400 [==============================] - 24s 60ms/step - loss: 0.4994 - wcceOA: 0.5018 - entrONA: 0.4781 - waccOA: 0.8083 - val_loss: 0.5324 - val_wcceOA: 0.5389 - val_entrONA: 0.4733 - val_waccOA: 0.8126\n",
      "\n",
      "Epoch 00058: val_waccOA did not improve from 0.86412\n",
      "Epoch 59/1000\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.5282 - wcceOA: 0.5353 - entrONA: 0.4645 - waccOA: 0.7927 - val_loss: 0.5766 - val_wcceOA: 0.5966 - val_entrONA: 0.3972 - val_waccOA: 0.6806\n",
      "\n",
      "Epoch 00059: val_waccOA did not improve from 0.86412\n",
      "Epoch 60/1000\n",
      "400/400 [==============================] - 10s 25ms/step - loss: 0.5040 - wcceOA: 0.5061 - entrONA: 0.4845 - waccOA: 0.8064 - val_loss: 0.4891 - val_wcceOA: 0.4864 - val_entrONA: 0.5131 - val_waccOA: 0.7876\n",
      "\n",
      "Epoch 00060: val_waccOA did not improve from 0.86412\n",
      "Epoch 61/1000\n",
      "400/400 [==============================] - 10s 26ms/step - loss: 0.5714 - wcceOA: 0.5835 - entrONA: 0.4616 - waccOA: 0.7572 - val_loss: 0.5090 - val_wcceOA: 0.5091 - val_entrONA: 0.5083 - val_waccOA: 0.7747\n",
      "\n",
      "Epoch 00061: val_waccOA did not improve from 0.86412\n",
      "Epoch 62/1000\n",
      "400/400 [==============================] - 9s 23ms/step - loss: 0.4950 - wcceOA: 0.4964 - entrONA: 0.4828 - waccOA: 0.8039 - val_loss: 0.5281 - val_wcceOA: 0.5299 - val_entrONA: 0.5122 - val_waccOA: 0.77050.4939 - wcceOA: 0.4952 - entrONA: 0.4824 - waccO\n",
      "\n",
      "Epoch 00062: val_waccOA did not improve from 0.86412\n",
      "Epoch 63/1000\n",
      "400/400 [==============================] - 9s 22ms/step - loss: 0.5165 - wcceOA: 0.5191 - entrONA: 0.4928 - waccOA: 0.7920 - val_loss: 0.6584 - val_wcceOA: 0.6945 - val_entrONA: 0.3336 - val_waccOA: 0.7376\n",
      "\n",
      "Epoch 00063: val_waccOA did not improve from 0.86412\n",
      "Epoch 64/1000\n",
      "400/400 [==============================] - 9s 23ms/step - loss: 0.5252 - wcceOA: 0.5353 - entrONA: 0.4341 - waccOA: 0.7836 - val_loss: 0.4870 - val_wcceOA: 0.4886 - val_entrONA: 0.4726 - val_waccOA: 0.8455\n",
      "\n",
      "Epoch 00064: val_waccOA did not improve from 0.86412\n",
      "Epoch 65/1000\n",
      "400/400 [==============================] - 9s 22ms/step - loss: 0.5099 - wcceOA: 0.5126 - entrONA: 0.4855 - waccOA: 0.8051 - val_loss: 0.5308 - val_wcceOA: 0.5364 - val_entrONA: 0.4804 - val_waccOA: 0.7541\n",
      "\n",
      "Epoch 00065: val_waccOA did not improve from 0.86412\n",
      "Epoch 66/1000\n",
      "400/400 [==============================] - 8s 21ms/step - loss: 0.5219 - wcceOA: 0.5248 - entrONA: 0.4957 - waccOA: 0.7851 - val_loss: 0.5322 - val_wcceOA: 0.5363 - val_entrONA: 0.4959 - val_waccOA: 0.7680\n",
      "\n",
      "Epoch 00066: val_waccOA did not improve from 0.86412\n",
      "Epoch 67/1000\n",
      "400/400 [==============================] - 9s 23ms/step - loss: 0.5365 - wcceOA: 0.5430 - entrONA: 0.4783 - waccOA: 0.7725 - val_loss: 0.4995 - val_wcceOA: 0.4993 - val_entrONA: 0.5014 - val_waccOA: 0.7937\n",
      "\n",
      "Epoch 00067: val_waccOA did not improve from 0.86412\n",
      "Epoch 68/1000\n",
      "400/400 [==============================] - 9s 22ms/step - loss: 0.4862 - wcceOA: 0.4867 - entrONA: 0.4811 - waccOA: 0.8070 - val_loss: 0.6146 - val_wcceOA: 0.6440 - val_entrONA: 0.3498 - val_waccOA: 0.6597\n",
      "\n",
      "Epoch 00068: val_waccOA did not improve from 0.86412\n",
      "Epoch 69/1000\n",
      "400/400 [==============================] - 9s 22ms/step - loss: 0.5162 - wcceOA: 0.5220 - entrONA: 0.4640 - waccOA: 0.7875 - val_loss: 0.5235 - val_wcceOA: 0.5257 - val_entrONA: 0.5037 - val_waccOA: 0.7692\n",
      "\n",
      "Epoch 00069: val_waccOA did not improve from 0.86412\n",
      "Epoch 70/1000\n",
      "400/400 [==============================] - 9s 22ms/step - loss: 0.5070 - wcceOA: 0.5106 - entrONA: 0.4737 - waccOA: 0.7892 - val_loss: 0.4806 - val_wcceOA: 0.4785 - val_entrONA: 0.4996 - val_waccOA: 0.8032\n",
      "\n",
      "Epoch 00070: val_waccOA did not improve from 0.86412\n",
      "Epoch 71/1000\n",
      "400/400 [==============================] - 9s 22ms/step - loss: 0.5142 - wcceOA: 0.5180 - entrONA: 0.4805 - waccOA: 0.7908 - val_loss: 0.5949 - val_wcceOA: 0.6205 - val_entrONA: 0.3642 - val_waccOA: 0.6621\n",
      "\n",
      "Epoch 00071: val_waccOA did not improve from 0.86412\n",
      "Epoch 72/1000\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.5545 - wcceOA: 0.5661 - entrONA: 0.4495 - waccOA: 0.7614 - val_loss: 0.4813 - val_wcceOA: 0.4792 - val_entrONA: 0.5005 - val_waccOA: 0.8000\n",
      "\n",
      "Epoch 00072: val_waccOA did not improve from 0.86412\n",
      "Epoch 73/1000\n",
      "400/400 [==============================] - 10s 25ms/step - loss: 0.5221 - wcceOA: 0.5261 - entrONA: 0.4859 - waccOA: 0.7952 - val_loss: 0.4943 - val_wcceOA: 0.4936 - val_entrONA: 0.5004 - val_waccOA: 0.7910\n",
      "\n",
      "Epoch 00073: val_waccOA did not improve from 0.86412\n",
      "Epoch 74/1000\n",
      "400/400 [==============================] - 9s 23ms/step - loss: 0.4961 - wcceOA: 0.4993 - entrONA: 0.4673 - waccOA: 0.8029 - val_loss: 0.5116 - val_wcceOA: 0.5134 - val_entrONA: 0.4953 - val_waccOA: 0.7739\n",
      "\n",
      "Epoch 00074: val_waccOA did not improve from 0.86412\n",
      "Epoch 75/1000\n",
      "400/400 [==============================] - 11s 28ms/step - loss: 0.5162 - wcceOA: 0.5225 - entrONA: 0.4592 - waccOA: 0.7858 - val_loss: 0.4568 - val_wcceOA: 0.4523 - val_entrONA: 0.4975 - val_waccOA: 0.8206\n",
      "\n",
      "Epoch 00075: val_waccOA did not improve from 0.86412\n",
      "Epoch 76/1000\n",
      "400/400 [==============================] - 11s 26ms/step - loss: 0.4824 - wcceOA: 0.4841 - entrONA: 0.4678 - waccOA: 0.8136 - val_loss: 0.4979 - val_wcceOA: 0.4985 - val_entrONA: 0.4926 - val_waccOA: 0.8171\n",
      "\n",
      "Epoch 00076: val_waccOA did not improve from 0.86412\n",
      "Epoch 77/1000\n",
      "400/400 [==============================] - 12s 29ms/step - loss: 0.5127 - wcceOA: 0.5173 - entrONA: 0.4713 - waccOA: 0.7977 - val_loss: 0.4845 - val_wcceOA: 0.4841 - val_entrONA: 0.4882 - val_waccOA: 0.7902\n",
      "\n",
      "Epoch 00077: val_waccOA did not improve from 0.86412\n",
      "Epoch 78/1000\n",
      "400/400 [==============================] - 17s 42ms/step - loss: 0.5217 - wcceOA: 0.5278 - entrONA: 0.4668 - waccOA: 0.7853 - val_loss: 0.4928 - val_wcceOA: 0.4939 - val_entrONA: 0.4826 - val_waccOA: 0.7936\n",
      "\n",
      "Epoch 00078: val_waccOA did not improve from 0.86412\n",
      "Epoch 79/1000\n",
      "400/400 [==============================] - 11s 27ms/step - loss: 0.5162 - wcceOA: 0.5230 - entrONA: 0.4548 - waccOA: 0.7923 - val_loss: 0.6146 - val_wcceOA: 0.6358 - val_entrONA: 0.4236 - val_waccOA: 0.7183\n",
      "\n",
      "Epoch 00079: val_waccOA did not improve from 0.86412\n",
      "Epoch 80/1000\n",
      "400/400 [==============================] - 13s 31ms/step - loss: 0.5051 - wcceOA: 0.5087 - entrONA: 0.4729 - waccOA: 0.8038 - val_loss: 0.4928 - val_wcceOA: 0.4933 - val_entrONA: 0.4882 - val_waccOA: 0.8063\n",
      "\n",
      "Epoch 00080: val_waccOA did not improve from 0.86412\n",
      "Epoch 00080: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1abfb8bd9a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import AFLungNet as LN\n",
    "import uuid\n",
    "import os\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# save model image\n",
    "modeldir = 'model-'+str(uuid.uuid4())\n",
    "print(modeldir)\n",
    "if not os.path.exists(modeldir):\n",
    "    os.makedirs(modeldir)\n",
    "open(modeldir+'/architecture.json', 'w').write(ensemble.to_json())\n",
    "plot_model(ensemble, to_file=modeldir+'/model.png', show_shapes=True)\n",
    "\n",
    "train_gen = LN.modified_sample_generator('lidc-train.npz')\n",
    "val_gen = LN.modified_sample_generator('lidc-val.npz', augment=False)\n",
    "\n",
    "checkpoints = ModelCheckpoint(modeldir+'/weights.{epoch:02d}-{val_waccOA:.2f}.hdf5', \n",
    "                              monitor='val_waccOA', verbose=1, mode='max',\n",
    "                              save_best_only=True)\n",
    "earlystopping = EarlyStopping(monitor='val_waccOA', min_delta=0.001, patience=40, \n",
    "                              verbose=1, mode='max')\n",
    "logger = CSVLogger(modeldir+'/training.log')\n",
    "\n",
    "# fit model\n",
    "ensemble.fit_generator(train_gen, 400, 1000, validation_data=val_gen, validation_steps=200,\n",
    "                    callbacks=[checkpoints, earlystopping, logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podsumowanie\n",
    "Najlepszy wynik jaki osiągneliśmy po połączeniu na zbiorze walidacyjnym to 0.86 ważonej celności. Zapewne wynika to, z nie najlepszego wyniku sieci *U-Net*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
