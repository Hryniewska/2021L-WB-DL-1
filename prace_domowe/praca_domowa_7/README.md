## Praca domowa nr 7 - Zaawansowane techniki

#### 1. Monitorowanie uczenia modeli

Podłączenie Tensorboard i śledź wskaźniki treningowe, pokaż wykresy modelowe.

Dodanie monitorowania do sieci (min. 2 z [dostępnych tutaj](https://keras.io/api/callbacks)).

Wymyśl i napisz jedno własne wywołanie zwrotne. Uzasadnij, dlaczego coś takiego przyda się w Twoim rozwiązaniu.


#### 2. Wiele wejść / wiele wyjść

Zaimplementuj jedno z poniższych:

  * Multimodalne wejście
  * Dodaj drugie wyjście do sieci

Napisz dlaczego dodałeś takie wejście lub wyjście.


#### 3. Ensemble

Zbuduj zespół modeli. Jeśli w Twojej pracy nie było trenowanych kilku modeli, to zależy mi na umiejętności implementacji, a nie zasadności połączenia tych modeli.


#### 4. Przygotuj prezentację (5-10 min)
Przygotować się na pytania.

Jeśli na końcu linii pojawia się gwiazdka (*) to proszę o przedstawienie, jak wyliczać liczbę parametrów, rozmiar następnej warstwy i [głębokość sieci](https://keras.io/api/applications/).<br /> 
Jeśli jedna architektura sieci ma kilka odmian np. VGG-16 lub VGG-19 to powiedz czym się one różnią, tak samo jeśli sieć ma kilka wersji np. Inception v1, Inception v2.

**13 maja**<br />
[Mikołaj Spytek] komórka LSTM<br /> 
[Tomasz Nocoń] komórka GRU<br />
[Bartłomiej Eljasiak] R-CNN<br />
[Zuzanna Mróz] Restricted Boltzmann machine<br />

**20 maja**<br />
[Kacper Kurowski] LeNet, AlexNet, VGG * <br />
[Paweł Koźmiński] self organizing maps, można na przykładzie sieci Kohonena<br />
[Dominik Pawlak] Siamese Network<br />
[Aleksander Podsiad] Attention Module<br />

**10 czerwca**<br />
[Piotr Marciniak] ResNet, DenseNet *<br />
[Piotr Piątyszek] Inception<br /> 
[Jakub Kozieł] Xception, MobileNet<br />
[Adam Frej] Fully Convolutional Network, DeconvNet<br />
[Maria Kałuska] bidirectional RNN<br />
[Tomasz Krupiński] autoenkoder wariacyjny VAE<br />
[Kacper Staroń] Mask R-CNN<br />

----

**Konwolucyjne sieci neuronowe**<br />
LeNet, AlexNet, VGG * <br />
ResNet, DenseNet *<br />
Inception<br /> 
Xception, MobileNet<br />
Fully Convolutional Network, DeconvNet

**Rekurencyjne sieci neuronowe**<br /> 
komórka LSTM<br /> 
komórka GRU<br />
bidirectional RNN

**Uczenie nienadzorowane**<br /> 
self organizing maps, można na przykładzie sieci Kohonena<br />
Restricted Boltzmann machine<br /> 
autoenkoder wariacyjny VAE

**Różności**<br /> 
Siamese Network<br />
R-CNN<br />
Mask R-CNN<br />
Attention Module
